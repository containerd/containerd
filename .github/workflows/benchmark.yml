name: benchmark

env:
  # Go version we currently use to build containerd across all CI.
  # FIXME make this version available on the self hosted runner
  GO_VERSION: '1.19'
  GOCACHE: /home/ubuntu/go/cache
  AWS_EC2_IMAGE_ID: ami-00f78bca13a3ed742
  AWS_EC2_INSTANCE_TYPE: t2.micro
  AWS_SUBNET_ID: subnet-063672503b1a6cebc
  AWS_SECURITY_GROUP_ID: sg-02118687bc0d8cbda

# Run on push to any branch
# Production use could restrict to main branch, and/or include pull_request
on: push

jobs:
  # TODO keep runner online, re-use it for additional workflow runs, suspend itself after idle timeout
  start-runner:
    name: Start self-hosted EC2 runner
    runs-on: ubuntu-latest
    outputs:
      label: ${{ steps.start-ec2-runner.outputs.label }}
      ec2-instance-id: ${{ steps.start-ec2-runner.outputs.ec2-instance-id }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Start EC2 runner
        id: start-ec2-runner
        uses: machulav/ec2-github-runner@v2
        with:
          mode: start
          github-token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          ec2-image-id: ${{ env.AWS_EC2_IMAGE_ID }}
          ec2-instance-type: ${{ env.AWS_EC2_INSTANCE_TYPE }}
          subnet-id: ${{ env.AWS_SUBNET_ID }}
          security-group-id: ${{ env.AWS_SECURITY_GROUP_ID }}

  bench:
    needs: start-runner
    runs-on: ${{ needs.start-runner.outputs.label }}
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Check out the source
        uses: actions/checkout@v3
      - name: Run benchmarks
        run: |
          mkdir -p "${{ runner.temp	}}/current" && sudo -E make bench-integration > "${{ runner.temp	}}/current/BenchmarkResults"
      - name: Upload the new benchmark results to an artifact
        uses: actions/upload-artifact@v3
        with:
          name: BenchmarkResults
          path: ${{ runner.temp	}}/current/BenchmarkResults
      - name: Export benchmark name list to job output
        id: set-matrix
        run: echo "::set-output name=matrix::$(jq -c '.benchmarkResults | keys' "${{ runner.temp }}/current/BenchmarkResults")"

  stop-runner:
    name: Stop self-hosted EC2 runner
    needs:
      - start-runner # required to get output from the start-runner job
      - bench # required to wait when the main job is done
    runs-on: ubuntu-latest
    if: ${{ always() }} # required to stop the runner even if the error happened in the previous jobs
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Stop EC2 runner
        uses: machulav/ec2-github-runner@v2
        with:
          mode: stop
          github-token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          label: ${{ needs.start-runner.outputs.label }}
          ec2-instance-id: ${{ needs.start-runner.outputs.ec2-instance-id }}

# this job will run once per benchmark and compare the current and previous values
  compare:
    needs: bench
    runs-on: ubuntu-latest
    # parallelize this job per benchmark
    strategy:
        matrix:
            benchmark: ${{ fromJson(needs.bench.outputs.matrix) }}
        # don't cancel other benchmarks when one fails
        fail-fast: false
  
    steps:
      - name: Download benchmark results from earlier in this workflow
        uses: actions/download-artifact@v3
        with:
          name: BenchmarkResults
          path: ${{ runner.temp	}}/current
      - name: Download benchmark results from the last completed workflow
        # TODO smarter error handling
        continue-on-error: true
        uses: dawidd6/action-download-artifact@v2
        with:
          ### FIXME
          ### success is too narrow if we fail the job when there are regressions
          ### completed is too broad, it will select a run no or defective artifacts
          workflow_conclusion: completed
          # branch main
          # FIXME don't compare across unrelated branches
          event: push
          name: BenchmarkResults
          path: ${{ runner.temp	}}/previous
      - name: Check out benchmark comparison script
        uses: snow-actions/sparse-checkout@v1.1.0
        with:
          patterns: |
            integration/benchmark/compare_results.jq
      - name: Compare current and previous bencmarks
        # TODO smarter error handling
        continue-on-error: true
        # only run this step if all previous steps completed without error
        if: ${{ success() }}
        # use jq to produce a comparison between the previous and current benchmarks
        run: >
          jq
          --argfile prev ${{ runner.temp }}/previous/BenchmarkResults
          --argfile curr ${{ runner.temp }}/current/BenchmarkResults
          --arg benchmark ${{ matrix.benchmark }}
          -n
          -f integration/benchmark/compare_results.jq
          | tee "${{ runner.temp }}/BenchmarkResultsPctDelta"
      
      # sanitize and store the comparison results to a step output that can be read by the later steps
      - name: Save comparison results
        if: ${{ success() }}
        id: results
        run: |
          DELTA="$(cat ${{ runner.temp }}/BenchmarkResultsPctDelta)"
          DELTA="${DELTA//'%'/%25}"
          DELTA="${DELTA//$'\n'/%0A}"   
          DELTA="${DELTA//$'\r'/%0D}"
          echo "::set-output name=delta::$DELTA"

      # TODO move regression checks to script in the repo for cleaner and more reusable code
      # TODO OR store delta to job output, matrix-ize regression check jobs to deduplicate code
      - name: Regression Mean
        if: always() && fromJSON(steps.results.outputs.delta).mean > 4
        uses: actions/github-script@v3 # provides better custom error output than `echo "::error::" && exit 1`
        with:
          script: |
            core.setFailed('Mean increased by ${{ fromJSON(steps.results.outputs.delta).mean }}%')

      - name: Regression Median
        if: always() && fromJSON(steps.results.outputs.delta).pct50 > 4
        uses: actions/github-script@v3
        with:
          script: |
            core.setFailed('Median increased by ${{ fromJSON(steps.results.outputs.delta).pct50 }}%')

      - name: Regression 90th Percentile
        if: always() && fromJSON(steps.results.outputs.delta).pct90 > 4
        uses: actions/github-script@v3
        with:
          script: |
            core.setFailed('90th Percentile increased by ${{ fromJSON(steps.results.outputs.delta).pct90 }}%')
